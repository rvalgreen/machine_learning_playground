{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW,DataCollatorWithPadding, AutoTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizer, BertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments,AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset as HFDataset, load_metric\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed) if torch.cuda.is_available() else None\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom PyTorch Dataset\n",
    "class EncodedDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import RAdam, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, \\\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup, get_constant_schedule_with_warmup\n",
    "\n",
    "# Choose optimizer and learning rate scheduler\n",
    "def getOptimizer(model, total_train_steps, scheduler_type=\"linear\",\n",
    "                  lr=1e-4, weight_decay=0.01, warmup_steps=0):\n",
    "    \n",
    "    optimizer = AdamW(params=model.parameters(), lr=float(lr), weight_decay=weight_decay)\n",
    "    \n",
    "    if scheduler_type == \"linear\":\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_train_steps\n",
    "        )\n",
    "    elif scheduler_type == \"cosine\":\n",
    "        lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_train_steps\n",
    "        )\n",
    "    else:\n",
    "        lr_scheduler = get_constant_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=warmup_steps\n",
    "        )\n",
    "\n",
    "    return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISTILLBERT TIMESPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "dataFrame = pd.read_csv('digitalizações_registadas.csv', \n",
    "                       delimiter=\";\",\n",
    "                       date_format=\"\", \n",
    "                       parse_dates=['Data Emissão','Data vencimento indicada',\"Data entrada\"]) \n",
    "\n",
    "# Strip any leading or trailing whitespace from column names\n",
    "dataFrame.columns = dataFrame.columns.str.strip()\n",
    "\n",
    "# Get unnamed columns to remove\n",
    "unnamed_columns = [col for col in dataFrame.columns if col.startswith('Unnamed')]\n",
    "\n",
    "# Drop unnamed columns\n",
    "dataFrame = dataFrame.drop(columns=unnamed_columns)\n",
    "\n",
    "# Drop rows with any null values\n",
    "dataFrame = dataFrame.dropna(subset=['Data vencimento indicada','Data Emissão','Origem']) #'Contrato'\n",
    "\n",
    "# Convert columns to date type\n",
    "dataFrame['Data entrada'] = pd.to_datetime(dataFrame['Data entrada'], format=\"%d/%m/%Y\")\n",
    "#dataFrame['Data Emissão'] = pd.to_datetime(dataFrame['Data Emissão'], format=\"%d/%m/%Y\")\n",
    "#dataFrame['Data vencimento indicada'] = pd.to_datetime(dataFrame['Data vencimento indicada'], format=\"%d/%m/%Y\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we only want to predict Contrato or Manual, we discard rows with Requisição\n",
    "dataFrame = dataFrame[dataFrame['Origem'] != \"Requisição\"]\n",
    "\n",
    "# Set Labels column (this is unecessary as we can use Origem - but good for readability)\n",
    "dataFrame['Labels'] = dataFrame['Origem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the labels in column B\n",
    "#grouped = dataFrame.groupby('Labels')\n",
    "\n",
    "# Determine the size of the smaller group\n",
    "#min_size = grouped.size().min()\n",
    "\n",
    "# Sample the smaller group size from each group and concatenate the results\n",
    "#dataFrame = grouped.apply(lambda x: x.sample(min_size)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"Fornecedor\",\"Data Emissão\",\"Data entrada\",\"Data vencimento indicada\", \"Valor com IVA\"]\n",
    "# Function to format the row data\n",
    "def format_row(row, feature_columns, dataframe):\n",
    "    parts = []\n",
    "    for column in feature_columns:\n",
    "        value = row[column]\n",
    "        if pd.api.types.is_datetime64_any_dtype(dataframe[column]):\n",
    "            value = value.strftime('%d/%m/%Y')\n",
    "        parts.append(f\"{column}: {value}\")\n",
    "    return '\\n '.join(parts) + \"\\n\"\n",
    "\n",
    "\n",
    "def buildDocRepresentation(dataframe, feature_columns):\n",
    "    dataframe['FullText'] = dataframe.apply(lambda row: format_row(row, feature_columns, dataframe), axis = 1)\n",
    "\n",
    "\n",
    "buildDocRepresentation(dataFrame, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build column with doc text representation\n",
    "dataFrame['FullText'] = (\n",
    "\"Fornecedor:\"+dataFrame['Fornecedor'] \n",
    "+ '\\n Data emissão:' + dataFrame['Data Emissão']  \n",
    "+ '\\n Data entrada:' + dataFrame['Data entrada'].dt.strftime('%d/%m/%Y')\n",
    "+ '\\n Data vencimento:' + dataFrame['Data vencimento indicada']\n",
    "+ \"\\n Valor com IVA:\"+dataFrame[\"Valor com IVA\"]\n",
    "+ \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Perform timesplit - we train on older samples and test on recent ones\n",
    "dataFrame_before_2024 = dataFrame[dataFrame['Data entrada'] < '2024-02-01']\n",
    "dataFrame_after_2024 = dataFrame[dataFrame['Data entrada'] >= '2024-02-01']\n",
    "\n",
    "# Especify what is train/test for readability\n",
    "train_texts = dataFrame_before_2024['FullText'].tolist()\n",
    "test_texts = dataFrame_after_2024['FullText'].tolist()\n",
    "train_labels = dataFrame_before_2024['Labels'].tolist()\n",
    "test_labels = dataFrame_after_2024['Labels'].tolist()\n",
    "\n",
    "# Encode labels - model cant take actual text - we need to encode text to numbers\n",
    "encoded_labels_train = label_encoder.fit_transform(train_labels)\n",
    "encoded_labels_test = label_encoder.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Contrato rows and Manual rows\n",
    "count_contrato = dataFrame_before_2024['Labels'].value_counts().get('Contrato', 0)\n",
    "count_manual = dataFrame_before_2024['Labels'].value_counts().get('Manual', 0)\n",
    "print(count_contrato)\n",
    "print(count_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check classes/labels\n",
    "list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our tokenizer - SHOULD MATCH OUR CHOSEN MODEL!\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Add special tokens if necessary: in this case we add a PAD token\n",
    "# to pad our input bc they must have the same length\n",
    "special_tokens_dict = {\"pad_token\": \"<PAD>\"}\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode our document text representations\n",
    "encodings_train = tokenizer(train_texts, truncation=True, padding=True, max_length=128 )\n",
    "encodings_test = tokenizer(test_texts, truncation=True, padding=True, max_length=128 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our encoded pytorch datasets\n",
    "train_dataset = EncodedDataset({'input_ids': encodings_train['input_ids'], \n",
    "                                'attention_mask': encodings_train['attention_mask']}, \n",
    "                                encoded_labels_train)\n",
    "\n",
    "val_dataset = EncodedDataset({'input_ids': encodings_test['input_ids'],\n",
    "                               'attention_mask': encodings_test['attention_mask']},\n",
    "                                 encoded_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataLoaders - responsible for building and preparing the batches\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\n",
    "    \"runName\":\"distillbert-mps-timesplit\",\n",
    "    \"lr\":1e-4,\n",
    "    \"epochs\":1,\n",
    "    \"weight_decay\":0.01,\n",
    "    \"save\":True,\n",
    "    \"scheduler_type\":\"cosine\",\n",
    "    \"warmup_steps\":100\n",
    "    },   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conf in configs:\n",
    "    # Instantiate the model\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))  # Adjust num_labels as needed\n",
    "\n",
    "    # move model to device\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate total train steps for scheduler\n",
    "    num_training_steps = len(train_loader) * conf['epochs']\n",
    "\n",
    "    # Init optimizer and scheduler\n",
    "    optimizer, scheduler = getOptimizer(model, num_training_steps, scheduler_type=conf['scheduler_type'],\n",
    "                                        lr=conf['lr'], weight_decay=conf['weight_decay'],\n",
    "                                        warmup_steps=conf['warmup_steps'])\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    steps = 0\n",
    "\n",
    "    # Wandb init conf\n",
    "    runName = f\"experiment_{conf['runName']}\"\n",
    "    run = wandb.init(\n",
    "        project=\"cob-demo\",\n",
    "        name=runName, \n",
    "        config={\n",
    "            \"learning_rate\": conf['lr'],\n",
    "            \"epochs\": conf['epochs'],\n",
    "            \"weight_decay\":conf['weight_decay'],\n",
    "            \"scheduler_type\":conf['scheduler_type'],\n",
    "            \"warmup_steps\":conf['warmup_steps']\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(conf['epochs']):\n",
    "        epoch_train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            steps += 1\n",
    "\n",
    "            # get inputs\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # forward inputs through model\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            # calculate loss\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            # perform backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "            wandb.log({\"loss\": loss.item(), \"step\":steps, \"learning_rate\":scheduler.get_last_lr()[0]})\n",
    "\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        print(f\"Run: {conf['runName']}   |Epoch {epoch + 1}, Train Loss: {epoch_train_loss}\")\n",
    "\n",
    "\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            #outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "                \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    val_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss}, Accuracy: {accuracy}%\")\n",
    "    \n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    wandb.run.summary[\"report\"] = report\n",
    "    wandb.finish()\n",
    "    if conf['save']:\n",
    "        model.save_pretrained(\"distillbert/\"+runName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distillbert/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new data\n",
    "def predict(texts):\n",
    "    # Tokenize the input texts\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
    "    \n",
    "    # Move the model to the correct device (CPU or GPU)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    encodings = {key: val.to(device) for key, val in encodings.items()}\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "    \n",
    "    # Get the predicted class labels\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    return predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"\"\"\n",
    "\n",
    "\"\"\"]\n",
    "print(predict(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "new_texts = [\n",
    "    \"\"\"\n",
    "Título: REGT: Geração de relatórios\n",
    "Tipo: Feature\n",
    "Descrição: Na solução de registos de tempo, é necessario implementar a capacidade de imprimir 4 tipos de relatórios sobre os registos de tempo:\n",
    "\n",
    "1º Relatório - tempo por pessoa, por empresa, por mês\n",
    "- 3 colunas: Colaborador, Empresa, Percentagem de **tempo (sem ter em conta custo)** que essa pessoa gastou nessa empresa \n",
    "\n",
    "2º Relatório - tempo por direção, por empresa, por mês\n",
    "- 3 colunas: Direção, Empresa, Percentagem de **tempo (sem ter em conta custo)** que os colaboradores dessa direção gastaram nessa empresa\n",
    "\n",
    "3º Relatório - tempo por empresa, por mês\n",
    "- 3 colunas: Empresa, Percentagem **tempo (sem ter em conta custo)** que no geral (ou seja, por todos os colaboradores) foi gasto nessa empresa\n",
    "\n",
    "4º Relatório - Custo por pessoa até agora\n",
    "- 5 colunas: Empresa, Direção, Colaborador, **tempo (sem ter em conta custo)** que essa pessoa gastou nessa empresa (igual ao 1º), **Custo** que essa percentagem vale.\n",
    "\n",
    "Nota: Este ultimo relatório permite facilmente chegar ao salario de um colaborador, que era algo que eles não queriam. Se calhar vale a pena comentar isso com eles na próxima reunião?\n",
    "Empresa Mello\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Título: erro numa customização impede mostrar pesquisa de Documento\n",
    "Tipo: Bug Customização\n",
    "Descrição: Boa tarde,\n",
    "\n",
    "Como administrador tenho notado uma lentidão ou por vezes fico sem resposta quando faço uma pesquisa específica, neste caso tentei pesquisar pelo ID 2056368 e nunca foi apresentada a listagem.\n",
    "\n",
    "Obrigada,\n",
    "\n",
    "\n",
    "\n",
    "![Screenshot 2024-01-26 131917.png](recordm/instances/364669/files/2822/Screenshot%202024-01-26%20131917.png)\n",
    "![Screenshot 2024-01-26 171804.png](recordm/instances/364669/files/2822/Screenshot%202024-01-26%20171804.png)\n",
    "![Screenshot 2024-01-26 171953.png](recordm/instances/364669/files/2822/Screenshot%202024-01-26%20171953.png)\n",
    "Empresa: Mello\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Título: Cores erradas no calendário\n",
    "Tipo: Suporte\n",
    "Descrição: As cores nos eventos dos calendarios estão erradas, e deviam aparecer conforme o campo \"Cor\"\n",
    "que existe em cada instância.\n",
    "Empresa: Mello\n",
    "\"\"\"\n",
    "    \n",
    "]\n",
    "\n",
    "predicted_labels = predict(new_texts)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Function to plot weight distributions\n",
    "def plot_weight_distributions(model):\n",
    "    weights = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            weights.extend(param.cpu().detach().numpy().flatten())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(weights, bins=100, kde=True)\n",
    "    plt.title('Distribution of Weights')\n",
    "    plt.xlabel('Weight Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot weight magnitude by layer\n",
    "def plot_weight_magnitude_by_layer(model):\n",
    "    layer_names = []\n",
    "    weight_magnitudes = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            layer_names.append(name)\n",
    "            weight_magnitudes.append(param.abs().mean().item())\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(layer_names, weight_magnitudes)\n",
    "    plt.title('Average Absolute Weight Magnitude by Layer')\n",
    "    plt.xlabel('Average Absolute Weight Magnitude')\n",
    "    plt.ylabel('Layer')\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot attention weights\n",
    "def plot_attention_weights(model):\n",
    "    # Get the attention weights from the first layer (example)\n",
    "    attention_weights = model.distilbert.transformer.layer[0].attention.q_lin.weight.detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention_weights, cmap='viridis')\n",
    "    plt.title('Attention Weights from First Layer')\n",
    "    plt.xlabel('Head')\n",
    "    plt.ylabel('Weight Index')\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the distributions\n",
    "plot_weight_distributions(model)\n",
    "plot_weight_magnitude_by_layer(model)\n",
    "plot_attention_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISTILLBERT NORMAL SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "dataFrame = pd.read_csv('digitalizações_registadas.csv', #'mdados_docs_cob.csv' mdados_docs_all_16k.csv\n",
    "                       delimiter=\";\",\n",
    "                       date_format=\"\", \n",
    "                       parse_dates=['Data Emissão','Data vencimento indicada',\"Data entrada\"])  #, parse_dates=['DataEmissao']\n",
    "\n",
    "# Strip any leading or trailing whitespace from column names\n",
    "dataFrame.columns = dataFrame.columns.str.strip()\n",
    "\n",
    "# Get unnamed columns to remove\n",
    "unnamed_columns = [col for col in dataFrame.columns if col.startswith('Unnamed')]\n",
    "\n",
    "# Drop unnamed columns\n",
    "dataFrame = dataFrame.drop(columns=unnamed_columns)\n",
    "\n",
    "# Drop rows with any null values\n",
    "dataFrame = dataFrame.dropna(subset=['Data vencimento indicada','Data Emissão','Origem']) #'Contrato'\n",
    "\n",
    "dataFrame['Data entrada'] = pd.to_datetime(dataFrame['Data entrada'], format=\"%d/%m/%Y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = dataFrame[dataFrame['Origem'] != \"Requisição\"]\n",
    "dataFrame['Labels'] = dataFrame['Origem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame['FullText'] = (\n",
    "\"Fornecedor:\"+dataFrame['Fornecedor'] \n",
    "+ '\\n Data emissão:' + dataFrame['Data Emissão']  \n",
    "+ '\\n Data entrada:' + dataFrame['Data entrada'].dt.strftime('%d/%m/%Y')\n",
    "+ '\\n Data vencimento:' + dataFrame['Data vencimento indicada']\n",
    "+ \"\\n Valor com IVA:\"+dataFrame[\"Valor com IVA\"]\n",
    "+ \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the CSV has 'text' and 'label' columns\n",
    "texts = dataFrame['FullText'].tolist()\n",
    "labels = dataFrame['Labels'].tolist()\n",
    "\n",
    "# Encode labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "special_tokens_dict = {\"pad_token\": \"<PAD>\"}\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=128 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    encodings['input_ids'], encoded_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "train_masks, val_masks = train_test_split(\n",
    "    encodings['attention_mask'], test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = EncodedDataset({'input_ids': train_texts, 'attention_mask': train_masks}, train_labels)\n",
    "val_dataset = EncodedDataset({'input_ids': val_texts, 'attention_mask': val_masks}, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\n",
    "    \"runName\":\"distillbert-mps-v1\",\n",
    "    \"lr\":1e-4,\n",
    "    \"epochs\":1,\n",
    "    \"weight_decay\":0.01,\n",
    "    \"save\":True\n",
    "    },   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conf in configs:\n",
    "    # Instantiate the model\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))  # Adjust num_labels as needed\n",
    "\n",
    "    # move model to device\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "    learning_rate=conf['lr'],\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=conf['epochs'],              # number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size for training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=0,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=conf['weight_decay'],               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    report_to=\"wandb\",\n",
    "    #fp16=True,\n",
    "    use_mps_device=True,\n",
    "    seed=seed,\n",
    "    data_seed=seed\n",
    "    )\n",
    "\n",
    "    metric = load_metric(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(p):\n",
    "        return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
    "    \n",
    "\n",
    "    # Initialize the Trainer\n",
    "    # may require the usage of model_init to ensure\n",
    "    # reproducibility\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    wandb.init(project=\"cob-demo\")\n",
    "    wandb.run.name = conf['runName']\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(f\"Evaluation results: {eval_result}\")\n",
    "    \n",
    "\n",
    "    wandb.finish()\n",
    "    if conf['save']:\n",
    "        model.save_pretrained(\"distillbert/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
