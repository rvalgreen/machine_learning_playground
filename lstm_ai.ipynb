{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN COMMON HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DataCollatorWithPadding, GPT2Tokenizer, DistilBertForSequenceClassification, DistilBertModel, DistilBertTokenizer, TrainingArguments, Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM architecture\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, \n",
    "                            num_layers=layers,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Take the output of the last LSTM cell\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom PyTorch Dataset\n",
    "class EncodedDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import RAdam, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, \\\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup, get_constant_schedule_with_warmup\n",
    "\n",
    "# Choose optimizer and learning rate scheduler\n",
    "def getOptimizer(model, total_train_steps, scheduler_type=\"linear\",\n",
    "                  lr=1e-4, weight_decay=0.01, warmup_steps=0):\n",
    "    \n",
    "    optimizer = AdamW(params=model.parameters(), lr=float(lr), weight_decay=weight_decay)\n",
    "    \n",
    "    if scheduler_type == \"linear\":\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_train_steps\n",
    "        )\n",
    "    elif scheduler_type == \"cosine\":\n",
    "        lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_train_steps\n",
    "        )\n",
    "    else:\n",
    "        lr_scheduler = get_constant_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=warmup_steps\n",
    "        )\n",
    "\n",
    "    return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM TIMESPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "dataFrame = pd.read_csv('digitalizações_registadas.csv',\n",
    "                       delimiter=\";\",\n",
    "                       date_format=\"\", \n",
    "                       parse_dates=['Data Emissão','Data vencimento indicada'])  \n",
    "\n",
    "# Strip any leading or trailing whitespace from column names\n",
    "dataFrame.columns = dataFrame.columns.str.strip()\n",
    "\n",
    "# Get unnamed columns to remove\n",
    "unnamed_columns = [col for col in dataFrame.columns if col.startswith('Unnamed')]\n",
    "\n",
    "# Drop unnamed columns\n",
    "dataFrame = dataFrame.drop(columns=unnamed_columns)\n",
    "\n",
    "# Drop rows with any null values\n",
    "dataFrame = dataFrame.dropna(subset=['Data vencimento indicada','Data Emissão','Origem']) #'Contrato'\n",
    "\n",
    "# Convert columns to date type\n",
    "dataFrame['Data entrada'] = pd.to_datetime(dataFrame['Data entrada'], format=\"%d/%m/%Y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we only want to predict Contrato or Manual, we discard rows with Requisição\n",
    "dataFrame = dataFrame[dataFrame['Origem'] != \"Requisição\"]\n",
    "\n",
    "# Set Labels column (this is unecessary as we can use Origem - but good for readability)\n",
    "dataFrame['Labels'] = dataFrame['Origem']\n",
    "\n",
    "# Build column with doc text representation\n",
    "dataFrame['FullText'] = (\n",
    "\"Fornecedor:\"+dataFrame['Fornecedor'] \n",
    "+ '\\n Data emissão:' + dataFrame['Data Emissão']  \n",
    "+ '\\n Data entrada:' + dataFrame['Data entrada'].dt.strftime('%d/%m/%Y')  \n",
    "+ '\\n Data vencimento:' + dataFrame['Data vencimento indicada']\n",
    "+ \"\\n Valor com IVA:\"+dataFrame[\"Valor com IVA\"]\n",
    "+ \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Perform timesplit - we train on older samples and test on recent ones\n",
    "dataFrame_before_2024 = dataFrame[dataFrame['Data entrada'] < '2024-02-01']\n",
    "dataFrame_after_2024 = dataFrame[dataFrame['Data entrada'] >= '2024-02-01']\n",
    "\n",
    "# Check lenght of splits\n",
    "print(len(dataFrame))\n",
    "print(len(dataFrame_before_2024))\n",
    "print(len(dataFrame_after_2024))\n",
    "\n",
    "# Especify what is train/test for readability\n",
    "train_texts = dataFrame_before_2024['FullText'].tolist()\n",
    "test_texts = dataFrame_after_2024['FullText'].tolist()\n",
    "train_labels = dataFrame_before_2024['Labels'].tolist()\n",
    "test_labels = dataFrame_after_2024['Labels'].tolist()\n",
    "\n",
    "# Encode labels - model cant take actual text - we need to encode text to numbers\n",
    "encoded_labels_train = label_encoder.fit_transform(train_labels)\n",
    "encoded_labels_test = label_encoder.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our tokenizer - SHOULD MATCH OUR CHOSEN MODEL!\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Add special tokens if necessary: in this case we add a PAD token\n",
    "# to pad our input bc they must have the same length\n",
    "special_tokens_dict = {\"pad_token\": \"<PAD>\"}\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode our document text representations\n",
    "encodings_train = tokenizer(train_texts, truncation=True, padding=True, max_length=128 )\n",
    "encodings_test = tokenizer(test_texts, truncation=True, padding=True, max_length=128 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our pytorch datasets\n",
    "train_dataset = EncodedDataset({'input_ids': encodings_train['input_ids'], \n",
    "                                'attention_mask': encodings_train['attention_mask']}, \n",
    "                                encoded_labels_train)\n",
    "val_dataset = EncodedDataset({'input_ids': encodings_test['input_ids'],\n",
    "                               'attention_mask': encodings_test['attention_mask']},\n",
    "                                 encoded_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter dimensions\n",
    "vocab_size = len(tokenizer)\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "num_labels = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    # DEFAULT\n",
    "    {\n",
    "    \"runName\":\"lstm-mps-timesplit-Ev1\",\n",
    "    \"lr\":5e-4,\n",
    "    \"epochs\":3,\n",
    "    \"weight_decay\":0.00,\n",
    "    \"layers\":3,\n",
    "    \"scheduler_type\":\"linear\",\n",
    "    \"warmup_steps\":0\n",
    "    },   \n",
    "    {\n",
    "    \"runName\":\"lstm-mps-timesplit-Ev2\",\n",
    "    \"lr\":5e-5,\n",
    "    \"epochs\":3,\n",
    "    \"weight_decay\":0.00,\n",
    "    \"layers\":3,\n",
    "    \"scheduler_type\":\"linear\",\n",
    "    \"warmup_steps\":0\n",
    "    },\n",
    "    {\n",
    "    \"runName\":\"lstm-mps-timesplit-Ev3\",\n",
    "    \"lr\":5e-6,\n",
    "    \"epochs\":3,\n",
    "    \"weight_decay\":0.00,\n",
    "    \"layers\":3,\n",
    "    \"scheduler_type\":\"linear\",\n",
    "    \"warmup_steps\":0\n",
    "    },\n",
    "  \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conf in configs:\n",
    "    # Instantiate the model\n",
    "    model = LSTMModel(vocab_size, embed_dim, hidden_dim, num_labels, conf['layers'])\n",
    "    # Move model to device (mps or cuda) bc its faster\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Read some conf values\n",
    "    num_epochs = conf['epochs']\n",
    "    lr = conf['lr']\n",
    "    # Calculate total train steps for lr scheduler\n",
    "    total_train_steps = len(train_loader) * num_epochs\n",
    "\n",
    "    # Init optimizer and lr scheduler\n",
    "    optimizer, scheduler = getOptimizer(model, total_train_steps, scheduler_type=conf['scheduler_type'],\n",
    "                                        lr=conf['lr'], weight_decay=conf['weight_decay'],\n",
    "                                        warmup_steps=conf['warmup_steps'])\n",
    "\n",
    "    # Define loss function    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    steps = 0\n",
    "\n",
    "    # Wandb init\n",
    "    run = wandb.init(\n",
    "        project=\"cob-demo\",\n",
    "        name=f\"experiment_{conf['runName']}_layers_{conf['layers']}\", \n",
    "        config={\n",
    "            \"learning_rate\": conf['lr'],\n",
    "            \"epochs\": num_epochs,\n",
    "            \"layers\":conf['layers'],\n",
    "            \"weight_decay\":conf['weight_decay'],\n",
    "            \"scheduler_type\":conf['scheduler_type'],\n",
    "            \"warmup_steps\":conf['warmup_steps']\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            steps += 1\n",
    "\n",
    "            # Get inputs\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # Forward inputs\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Perform backward to update weights\n",
    "            loss.backward()\n",
    "            # Optimizer and scheduler steps\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # Zero gradients to avoid explosions\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            wandb.log({\"loss\": loss.item(), \"step\":steps, \"learning_rate\":scheduler.get_last_lr()[0]})\n",
    "\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        print(f\"Run: {conf['runName']}   |Epoch {epoch + 1}, Train Loss: {epoch_train_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        #outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        #_, predicted = torch.max(outputs.logits, 1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        #loss = criterion(outputs.logits, labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "                \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    val_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss}, Accuracy: {accuracy}%\")\n",
    "\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    wandb.run.summary[\"report\"] = report\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"lstm_mps.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"lstm_invoice_classifier.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_invoice_text = \"Example invoice text here.\"\n",
    "inputs = tokenizer(new_invoice_text, return_tensors='pt', truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "# Decode the predictions to get the class label\n",
    "predicted_label = label_encoder.inverse_transform(predictions.cpu().numpy())[0]\n",
    "\n",
    "print(f\"Predicted Contract ID: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM NORMAL SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "dataFrame = pd.read_csv('digitalizações_registadas.csv', #'mdados_docs_cob.csv' mdados_docs_all_16k.csv\n",
    "                       delimiter=\";\",\n",
    "                       date_format=\"\", \n",
    "                       parse_dates=['Data Emissão','Data vencimento indicada'])  #, parse_dates=['DataEmissao']\n",
    "\n",
    "# Strip any leading or trailing whitespace from column names\n",
    "dataFrame.columns = dataFrame.columns.str.strip()\n",
    "\n",
    "# Get unnamed columns to remove\n",
    "unnamed_columns = [col for col in dataFrame.columns if col.startswith('Unnamed')]\n",
    "\n",
    "# Drop unnamed columns\n",
    "dataFrame = dataFrame.drop(columns=unnamed_columns)\n",
    "\n",
    "# Drop rows with any null values\n",
    "dataFrame = dataFrame.dropna(subset=['Data vencimento indicada','Data Emissão','Origem']) #'Contrato'\n",
    "\n",
    "dataFrame['Data entrada'] = pd.to_datetime(dataFrame['Data entrada'], format=\"%d/%m/%Y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = dataFrame[dataFrame['Origem'] != \"Requisição\"]\n",
    "dataFrame['Labels'] = dataFrame['Origem']\n",
    "\n",
    "dataFrame['FullText'] = (\n",
    "\"Fornecedor:\"+dataFrame['Fornecedor'] \n",
    "+ '\\n Data emissão:' + dataFrame['Data Emissão']  \n",
    "+ '\\n Data entrada:' + dataFrame['Data entrada'].dt.strftime('%d/%m/%Y')  \n",
    "+ '\\n Data vencimento:' + dataFrame['Data vencimento indicada']\n",
    "+ \"\\n Valor com IVA:\"+dataFrame[\"Valor com IVA\"]\n",
    "+ \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the CSV has 'text' and 'label' columns\n",
    "texts = dataFrame['FullText'].tolist()\n",
    "labels = dataFrame['Labels'].tolist()\n",
    "\n",
    "# Encode labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "special_tokens_dict = {\"pad_token\": \"<PAD>\"}\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=128 ) #truncation=True, padding=True, max_length=128 , return_tensors='pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    encodings['input_ids'], encoded_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_masks, val_masks = train_test_split(\n",
    "    encodings['attention_mask'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = EncodedDataset({'input_ids': train_texts, 'attention_mask': train_masks}, train_labels)\n",
    "val_dataset = EncodedDataset({'input_ids': val_texts, 'attention_mask': val_masks}, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\n",
    "    \"runName\":\"lstm-mps-v1-A\",\n",
    "    \"lr\":1e-4,\n",
    "    \"epochs\":3,\n",
    "    \"weight_decay\":0.00,\n",
    "    \"layers\":3,\n",
    "    \"scheduler_type\":\"constant\",\n",
    "    \"warmup_steps\":0\n",
    "    },       \n",
    "    {\n",
    "    \"runName\":\"lstm-mps-v1-B\",\n",
    "    \"lr\":5e-4,\n",
    "    \"epochs\":3,\n",
    "    \"weight_decay\":0.00,\n",
    "    \"layers\":3,\n",
    "    \"scheduler_type\":\"constant\",\n",
    "    \"warmup_steps\":0\n",
    "    },   \n",
    "        {\n",
    "    \"runName\":\"lstm-mps-v1-C\",\n",
    "    \"lr\":1e-3,\n",
    "    \"epochs\":3,\n",
    "    \"weight_decay\":0.00,\n",
    "    \"layers\":3,\n",
    "    \"scheduler_type\":\"constant\",\n",
    "    \"warmup_steps\":0\n",
    "    },   \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conf in configs:\n",
    "    # Instantiate the model\n",
    "    model = LSTMModel(vocab_size, embed_dim, hidden_dim, num_labels, conf['layers'])\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    num_epochs = conf['epochs']\n",
    "    lr = conf['lr']\n",
    "    total_train_steps = len(train_loader) * num_epochs\n",
    "\n",
    "    optimizer, scheduler = getOptimizer(model, total_train_steps, scheduler_type=conf['scheduler_type'],\n",
    "                                        lr=conf['lr'], weight_decay=conf['weight_decay'],\n",
    "                                        warmup_steps=conf['warmup_steps'])\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    steps = 0\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"cob-demo\",\n",
    "        name=f\"experiment_{conf['runName']}_layers_{conf['layers']}\", \n",
    "        config={\n",
    "            \"learning_rate\": conf['lr'],\n",
    "            \"epochs\": num_epochs,\n",
    "            \"layers\":conf['layers'],\n",
    "            \"weight_decay\":conf['weight_decay'],\n",
    "            \"scheduler_type\":conf['scheduler_type'],\n",
    "            \"warmup_steps\":conf['warmup_steps']\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            steps += 1\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "            wandb.log({\"loss\": loss.item(), \"step\":steps, \"learning_rate\":scheduler.get_last_lr()[0]})\n",
    "\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        print(f\"Run: {conf['runName']}   |Epoch {epoch + 1}, Train Loss: {epoch_train_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            #outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            #_, predicted = torch.max(outputs.logits, 1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            #loss = criterion(outputs.logits, labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "                \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    val_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss}, Accuracy: {accuracy}%\")\n",
    "\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    wandb.run.summary[\"report\"] = report\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
